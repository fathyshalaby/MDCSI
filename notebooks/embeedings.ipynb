{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm \n",
    "import hickle as hkl\n",
    "def plot_embeedings(previous_model_name,trained_model_name,domain_trained_model_name,test_dataset,base_model_name=\"sentence-transformers/all-roberta-large-v1\",domain=None,setfitfusion_embeeding=None):\n",
    "    # Plot the embeedings of the previous model and the trained model to see the difference\n",
    "    # Load a SetFit model from Hub\n",
    "    # give a legend what the different colors which label is\n",
    "    labels = np.unique(test_dataset[\"label_name\"])\n",
    "    # use 14  very distinct colors\n",
    "   \n",
    "    base_model = sentence_transformers.SentenceTransformer(base_model_name)\n",
    "    previous_model = sentence_transformers.SentenceTransformer(previous_model_name)\n",
    "    trained_model =sentence_transformers.SentenceTransformer(trained_model_name)\n",
    "    domain_model = sentence_transformers.SentenceTransformer(domain_trained_model_name)\n",
    "    base_embeddings = base_model.encode(test_dataset[\"text\"])\n",
    "    prev_embeddings = previous_model.encode(test_dataset[\"text\"])\n",
    "    trained_embeddings = trained_model.encode(test_dataset[\"text\"])\n",
    "    domain_embeddings = domain_model.encode(test_dataset[\"text\"])\n",
    "    setfitfusion_embeedings = setfitfusion_embeeding\n",
    "    # Plot the embeedings of the previous model and the trained model to see the difference with color coding of the labels\n",
    "\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(20, 20))\n",
    "    # use a tsne to reduce the dimensionality of the embeddings\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    base_embeddings = tsne.fit_transform(base_embeddings)\n",
    "    prev_embeddings = tsne.fit_transform(prev_embeddings)\n",
    "    trained_embeddings = tsne.fit_transform(trained_embeddings)\n",
    "    domain_embeddings = tsne.fit_transform(domain_embeddings)\n",
    "    setfitfusion_embeedings = tsne.fit_transform(setfitfusion_embeedings)\n",
    "\n",
    "    # plot the embeddings\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(labels)))\n",
    "    handles = [plt.plot([], [], marker=\"o\", ls=\"\", color=colors[i])[0] for i in range(len(labels))]\n",
    "    ax[0].scatter(base_embeddings[:, 0], base_embeddings[:, 1], c=colors[test_dataset[\"label\"]])\n",
    "    ax[0].set_title(\"Base model\")\t\n",
    "    ax[1].scatter(prev_embeddings[:, 0], prev_embeddings[:, 1],c=colors[test_dataset[\"label\"]])\n",
    "    ax[1].set_title(\"Previous model\")\n",
    "    ax[2].scatter(trained_embeddings[:, 0], trained_embeddings[:, 1], c=colors[test_dataset[\"label\"]])\n",
    "    ax[2].set_title(\"Similar domain trained model\")\n",
    "    ax[3].scatter(domain_embeddings[:, 0], domain_embeddings[:, 1], c=colors[test_dataset[\"label\"]])\n",
    "    ax[3].set_title(\"Domain trained model\")\n",
    "    ax[4].scatter(setfitfusion_embeedings[:, 0], setfitfusion_embeedings[:, 1], c=colors[test_dataset[\"label\"]])\n",
    "    ax[4].set_title(\"setfitfusion\")\n",
    "    \n",
    "    fig.legend(handles, labels, bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    #SAVE THE FIGURE\n",
    "    fig.savefig(f\"{domain}-embeedings.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "def get_combo_name(domain):\n",
    "    df = pd.read_csv(\"setfit-soupres5new.csv\")\n",
    "    combo_names = df[\"Combo Name\"].values\n",
    "    domains = df[\"Domain\"].values\n",
    "    for i in range(len(domains)):\n",
    "        if domains[i] == domain:\n",
    "            return combo_names[i]\n",
    "\n",
    "for domain in tqdm([\n",
    "        \"oeffentlichkeit-soziales\",\n",
    "        \"supermaerkte-drogerien\",\n",
    "        \"mode-schmuck-zubehoer\",\n",
    "        \"moebel-einrichtungshaeuser\",\n",
    "        \"finanzen\",\n",
    "        \"reisen-tourismus\",\n",
    "        \"schoenheit-wellness\",\n",
    "        \"unternehmen-verbaende\",\n",
    "        \"medizin-gesundheit-pflege\",\n",
    "        \"transport-logistik\",\n",
    "        \"versicherungen-recht\",\n",
    "        \"oeffentlicher-verkehr-vermietung\",\n",
    "        \"unterhaltung-kultur-freizeit\",\n",
    "        \"wasser-strom-gas\",\n",
    "        \"haus-reinigung\",\n",
    "        \"full\"\n",
    "    ]):\n",
    "    try:\n",
    "        embeddings = hkl.load(f\"emb/{domain}_test_avgembedding.hkl\")\n",
    "        dataset = load_dataset(f\"fathyshalab/reklamation24_{domain}-v2\",use_auth_token=True)\n",
    "\n",
    "        # if test split is not present, then use validation split as the test split\n",
    "        if dataset.get(\"test\") is None and dataset.get(\"validation\") is None:\n",
    "            dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "            eval_dataset = dataset[\"test\"]\n",
    "\n",
    "        elif dataset.get(\"validation\") is None:\n",
    "            eval_dataset = dataset[\"test\"]\n",
    "\n",
    "        else:\n",
    "            eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "        plot_embeedings(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\"fathyshalab/reklambox-haus-reinigung-setfit\",f\"fathyshalab/reklambox-{domain}-setfit\",eval_dataset,domain=domain,setfitfusion_embeeding=embeddings[get_combo_name(domain)])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeeding(data,model):\n",
    "    labels = np.unique(data[\"label_text\"])\n",
    "    predictions = model.predict(data[\"text\"])\n",
    "    embeeding = model.encode(data[\"text\"])\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    embeeding = tsne.fit_transform(embeeding)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 15))\n",
    "    handles = [plt.plot([], [], marker=\"o\", ls=\"\", color=colors[i])[0] for i in range(len(labels))]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax[0].scatter(embeeding[:, 0], embeeding[:, 1], c=colors[data[\"label\"]])\n",
    "    ax[1].scatter(embeeding[:, 0], embeeding[:, 1], c=colors[predictions])\n",
    "    ax[0].legend(handles, labels)\n",
    "    ax[1].legend(handles, labels)\n",
    "    ax[0].set_title(\"True labels\")\n",
    "    ax[1].set_title(\"Predicted labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_centroid(model_name:str, dataset:str,n:int):\n",
    "    \"\"\"Get the centroid of each label and the closest n sentences to each centroid for more concious sampling\n",
    "    Args:\n",
    "        model_name (str): the name of the model to use from huggingface\n",
    "        dataset (str): the dataset to use from huggingface datasets\n",
    "        n (int): the number of closest sentences to each centroid that would be returned\n",
    "        Returns:\n",
    "        closest_sentences (dict): a dictionary with the label as key and the closest n sentences as values\n",
    "    \"\"\"\n",
    "    sentence2embedding = {}\n",
    "    model = sentence_transformers.SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(dataset[\"text\"])\n",
    "    #plot the embeddings\n",
    "    \n",
    "    for i in range(len(dataset[\"text\"])):\n",
    "        sentence2embedding[dataset[\"text\"][i]] = embeddings[i]\n",
    "    labels = np.unique(dataset[\"label\"])\n",
    "    dataset = dataset.to_pandas()\n",
    "    centroids = []\n",
    "    for label in labels:\n",
    "        label_embeddings = embeddings[dataset[\"label\"]==label]\n",
    "        centroid = np.mean(label_embeddings, axis=0)\n",
    "        centroids.append((centroid,label))\n",
    "    # find closest n sentence to each centroid\n",
    "    closest_sentences= {}\n",
    "    for centroid in centroids:\n",
    "        closest_sentences[centroid[1]] = []\n",
    "        label = centroid[1]\t\n",
    "        centroid = centroid[0]\n",
    "        for sentence, embedding in sentence2embedding.items():\n",
    "            # only consider sentences with the same label\n",
    "            if dataset[dataset[\"text\"]==sentence][\"label\"].values[0] != label:\n",
    "                continue\n",
    "            if np.isnan(embedding).any() or np.isnan(centroid).any():\n",
    "                continue  # skip embeddings with NaN values\n",
    "            sim = cosine_similarity(centroid.reshape(1, -1), embedding.reshape(1, -1))\n",
    "            closest_sentences[label].append((sentence,sim))\n",
    "        closest_sentences[label] = sorted(closest_sentences[label], key=lambda x: x[1], reverse=True)\n",
    "        closest_sentences[label]= [s[0] for s in closest_sentences[label][:n]]\n",
    "    return closest_sentences\n",
    "\n",
    "dataset = load_dataset(\"fathyshalab/reklambox-1\")\n",
    "\n",
    "# if test split is not present, then use validation split as the test split\n",
    "if dataset.get(\"test\") is None and dataset.get(\"validation\") is None:\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "elif dataset.get(\"validation\") is None:\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "else:\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "\n",
    "\n",
    "get_centroid(\"fathyshalab/clinic-kitchen_and_dining-roberta-domain-adaptation\",eval_dataset,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#producing a dataframe where the sentences are under the text column and the labels are under the label column from the dictionarz generated by the get_centroid function\n",
    "from datasets import Dataset\n",
    "def get_centroid_df(model_name:str, dataset:str,n:int):\n",
    "\n",
    "    closest_sentences = get_centroid(model_name, dataset,n)\n",
    "    df = pd.DataFrame(columns=[\"text\",\"label\"])\n",
    "    for label, sentences in closest_sentences.items():\n",
    "        for sentence in sentences:\n",
    "            df = df.append({\"text\":sentence,\"label\":label},ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def sample_dataset_centroid(model_name:str, dataset:str,n:int):\n",
    "    \"\"\"Sample the dataset based on the centroid of each label\n",
    "    Args:\n",
    "        model_name (str): the name of the model to use from huggingface\n",
    "        dataset (str): the dataset to use from huggingface datasets\n",
    "        n (int): the number of closest sentences to each centroid that would be returned\n",
    "        Returns:\n",
    "        sampled_dataset (dict): a dictionary with the label as key and the closest n sentences as values\n",
    "    \"\"\"\n",
    "    sampled_dataset = {}\n",
    "    closest_sentences = get_centroid_df(model_name, dataset,n)\n",
    "    # create a huggingface dataset from the dataframe\n",
    "    sampled_dataset = Dataset.from_pandas(closest_sentences)\n",
    "    return sampled_dataset\n",
    "\n",
    "dataset = load_dataset(\"fathyshalab/reklambox\")\n",
    "\n",
    "# if test split is not present, then use validation split as the test split\n",
    "if dataset.get(\"test\") is None and dataset.get(\"validation\") is None:\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "elif dataset.get(\"validation\") is None:\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "else:\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "sampled_dataset = sample_dataset_centroid(\"fathyshalab/clinic-kitchen_and_dining-roberta-domain-adaptation\",eval_dataset,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_closest_sentences(model_name, dataset, label, n):\n",
    "    sentence2embedding = {}\n",
    "    model = sentence_transformers.SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(dataset[\"text\"])\n",
    "    dataset = dataset.to_pandas()\n",
    "    for i in range(len(dataset[\"text\"])):\n",
    "        sentence2embedding[dataset[\"text\"][i]] = embeddings[i]\n",
    "    label_embeddings = embeddings[dataset[\"label_name\"]==label]\n",
    "    if len(label_embeddings) == 0:\n",
    "        raise ValueError(f\"No samples with label {label}\")\n",
    "    centroid = np.mean(label_embeddings,axis=0)\n",
    "    closest_sentences = []\n",
    "    for sentence, embedding in sentence2embedding.items():\n",
    "        if np.isnan(embedding).any() or np.isnan(centroid).any():\n",
    "            continue  # skip embeddings with NaN values\n",
    "        sim = cosine_similarity(centroid.reshape(-1,1), embedding.reshape(-1,1))\n",
    "        closest_sentences.append((sentence, sim))\n",
    "    closest_sentences = sorted(closest_sentences, key=lambda x: x[1], reverse=True)\n",
    "    return [s[0] for s in closest_sentences[:n]]\n",
    "model_name = \"fathyshalab/clinic-kitchen_and_dining-roberta-domain-adaptation\"\n",
    "label = \"austausch\"\n",
    "n = 5\n",
    "closest_sentences = get_closest_sentences(model_name, eval_dataset, label, n)\n",
    "print(closest_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i have the different setfusion embeedings in a hickle file, plot them and see how they are distributed compared to the original embeddings of the dataset\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# load the embeddings\n",
    "domain = \"oeffentlichkeit-soziales\"\n",
    "embeddings = hkl.load(f\"emb/{domain}_test_avgembedding.hkl\")\n",
    "#select the embeddings of the dataset which have the top scores based on the data in the csv file called setfit-soupres5new.csv, the option with the best score is in the combo name column\n",
    "\n",
    "df = pd.read_csv(\"setfit-soupres5new.csv\")\n",
    "combo_names = df[\"Combo Name\"].values\n",
    "# return me the combo name for the given domain, so find the row where domain is equal to the given domain and return the combo name of that row\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[get_combo_name(domain)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensitive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "085e1dbf50007ece7010906227ff88b0cb7f804d40cfb6f5dbda22b4107c8161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
